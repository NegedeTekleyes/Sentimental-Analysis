{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c99b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# setup and verification\n",
    "print(\"Intializing sentiment analysis project\")\n",
    "print(\"=\" *50)\n",
    "#  import packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gradio as gr\n",
    "import os\n",
    "from google.colab import drive\n",
    "#  verify envt\n",
    "print(\"Enviroment checked\")\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "  print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "  # print(f\"Gpu memory: {torch.cuda.get_device_properties(0).total_memort /1e9:.1f} GB\")\n",
    "  # create project structuere\n",
    "  os.makedirs('/content/models', exist_ok=True)\n",
    "  os.makedirs('/content/results', exist_ok=True)\n",
    "  print(\"Project directories created\")\n",
    "\n",
    "  print(\"Ready to start project\")\n",
    "  print(\"=\" *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f0af5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# data loading and exploration\n",
    "print(\"Loading and exploring dataset\")\n",
    "print(\"=\" *50)\n",
    "def load_and_explore_data():\n",
    "  \"\"\"Load IMDB dataset and provide comprehensive analysis\"\"\"\n",
    "  print(\"1. Downloading IMDB dataset...\")\n",
    "  dataset = load_dataset(\"imdb\")\n",
    "\n",
    "  print(\"2. Dataset structure:\")\n",
    "  print(f\" -Train samples: {len(dataset['train']):,}\")\n",
    "  print(f\" -Train samples: {len(dataset['test']):,}\")\n",
    "  print(f\" -Validation samples: {len(dataset['unsupervised']):,}\")\n",
    "\n",
    "  print(\"3. Sample data preview:\")\n",
    "  sample_data = dataset['train'].select(range(3))\n",
    "  for i, example in enumerate(sample_data):\n",
    "    print(f\" Sample{i+1}:\")\n",
    "    # Fix: Use example['text'] instead of sample['text']\n",
    "    print(f\" Text: {example['text'][:100]}...\")\n",
    "    print(f\" label: {example['label']} ({'Positive' if example['label'] == 1 else 'Negative'})\")\n",
    "    print()\n",
    "\n",
    "  # LABEL DISTRIBUTUION - Moved inside the function\n",
    "  train_labels = dataset['train']['label']\n",
    "  positive_count = sum(train_labels)\n",
    "  # Fix: calculate negative_count correctly\n",
    "  negative_count = len(train_labels) - positive_count\n",
    "  print(\"4. Label distribution:\")\n",
    "  print(f\"   - Positive reviews: {positive_count:,} ({positive_count/len(train_labels)*100:.1f}%)\")\n",
    "  print(f\"   - Negative reviews: {negative_count:,} ({negative_count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "  return dataset\n",
    "\n",
    "dataset = load_and_explore_data()\n",
    "print(\"Data set loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852aa80",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model intialization\n",
    "print(\"Intialize BERT model\")\n",
    "print(\"=\" *35)\n",
    "\n",
    "def initialize_bert_model():\n",
    "  \"\"\"Intializing DistilBERT model and tokenizer with professional setup\"\"\"\n",
    "  model_name=\"distilbert-base-uncase\"\n",
    "\n",
    "  print(f\"1. Loading tokenizer: {model_name}\")\n",
    "  tokenizer = AutoTokenizer.form_pretrained(model_name)\n",
    "\n",
    "  print(f\"2. Loading pre-trained model...\")\n",
    "  model = AutoModelForSequenceClassification.form_pretrained(model_name,\n",
    "                                                             num_labels=2,\n",
    "                                                             id2label={0: \"Negative\", 1: \"Positive\"},\n",
    "                                                             label2id={\"Negative\": 0, \"Positive\": 1}\n",
    "                                                             )\n",
    "  print(\"3. Model architecture overview:\")\n",
    "  print(\"f Model type: {model.__class__.__name__}\")\n",
    "  print(f\" Number of parameters: {model.num_parameters():,}\")\n",
    "  print(f\" Number of labels: {model.config.num_labels}\")\n",
    "  \n",
    "  print(\"4. Moving model to GPU...\")\n",
    "  if torch.cuda.is_available:\n",
    "    model=model.to('cuda')\n",
    "    print(\" Model succesfully moved to GPU\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "  # intialize model\n",
    "  tokenizer, model = initialize_bert_model()\n",
    "  print(\"BERT model intialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133960f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Data Preprocessing & Tokenization\n",
    "print(\"DATA PREPROCESSING & TOKENIZATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def preprocess_data(tokenizer, dataset):\n",
    "    \"\"\"Tokenize and prepare dataset for training\"\"\"\n",
    "    print(\"1. Defining tokenization function...\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Return as PyTorch tensors\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    print(\"2. Tokenizing training dataset...\")\n",
    "    tokenized_train = dataset[\"train\"].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000\n",
    "    )\n",
    "\n",
    "    print(\"3. Tokenizing test dataset...\")\n",
    "    tokenized_test = dataset[\"test\"].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000\n",
    "    )\n",
    "\n",
    "    print(\"4. Dataset overview after tokenization:\")\n",
    "    print(f\"   - Training features: {list(tokenized_train.features.keys())}\")\n",
    "\n",
    "    # --- Diagnosis: Inspect the output of tokenization for a single sample ---\n",
    "    print(\"\\n--- Diagnosis of tokenized output ---\")\n",
    "    first_example_input_ids = tokenized_train[0]['input_ids']\n",
    "    print(f\"Type of tokenized_train[0]['input_ids']: {type(first_example_input_ids)}\")\n",
    "    if isinstance(first_example_input_ids, list):\n",
    "        print(f\"Length of the list: {len(first_example_input_ids)}\")\n",
    "        if len(first_example_input_ids) > 0:\n",
    "            print(f\"Type of the first element in the list: {type(first_example_input_ids[0])}\")\n",
    "            # If the first element is a tensor, get its shape\n",
    "            if isinstance(first_example_input_ids[0], torch.Tensor):\n",
    "                 print(f\"Shape of the first element (tensor): {first_example_input_ids[0].shape}\")\n",
    "            else:\n",
    "                # If the first element is not a tensor, try to convert and print shape\n",
    "                try:\n",
    "                    temp_tensor = torch.tensor(first_example_input_ids[0])\n",
    "                    print(f\"Shape after converting first element to tensor: {temp_tensor.shape}\")\n",
    "                except:\n",
    "                    print(\"Could not convert the first element to a tensor.\")\n",
    "    else:\n",
    "        # If it's not a list, assume it's a tensor and print its shape\n",
    "        print(f\"Input shape: {first_example_input_ids.shape}\")\n",
    "    print(\"-------------------------------------\")\n",
    "    # --- End of Diagnosis ---\n",
    "\n",
    "    # Original line causing error - will be skipped in diagnosis phase\n",
    "    # print(f\"   - Input shape: {tokenized_train[0]['input_ids'].shape}\")\n",
    "\n",
    "\n",
    "    return tokenized_train, tokenized_test\n",
    "\n",
    "# Preprocess data\n",
    "tokenized_train, tokenized_test = preprocess_data(tokenizer, dataset)\n",
    "print(\"DATA PREPROCESSING COMPLETED\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
