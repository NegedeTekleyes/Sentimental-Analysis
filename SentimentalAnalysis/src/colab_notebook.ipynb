{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup and verification\n",
    "print(\"Intializing sentiment analysis project\")\n",
    "print(\"=\" *50)\n",
    "#  import packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import(\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import gradio as gr\n",
    "import os\n",
    "from google.colab import drive\n",
    "#  verify envt\n",
    "print(\"Enviroment checked\")\n",
    "print(f\"pytorch version: {torch.__version__}\")\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "  print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "  # print(f\"Gpu memory: {torch.cuda.get_device_properties(0).total_memort /1e9:.1f} GB\")\n",
    "  # create project structuere\n",
    "  os.makedirs('/content/models', exist_ok=True)\n",
    "  os.makedirs('/content/results', exist_ok=True)\n",
    "  print(\"Project directories created\")\n",
    "\n",
    "  print(\"Ready to start project\")\n",
    "  print(\"=\" *50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1f0af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading and exploration\n",
    "print(\"Loading and exploring dataset\")\n",
    "print(\"=\" *50)\n",
    "def load_and_explore_data():\n",
    "  \"\"\"Load IMDB dataset and provide comprehensive analysis\"\"\"\n",
    "  print(\"1. Downloading IMDB dataset...\")\n",
    "  dataset = load_dataset(\"imdb\")\n",
    "\n",
    "  print(\"2. Dataset structure:\")\n",
    "  print(f\" -Train samples: {len(dataset['train']):,}\")\n",
    "  print(f\" -Train samples: {len(dataset['test']):,}\")\n",
    "  print(f\" -Validation samples: {len(dataset['unsupervised']):,}\")\n",
    "\n",
    "  print(\"3. Sample data preview:\")\n",
    "  sample_data = dataset['train'].select(range(3))\n",
    "  for i, example in enumerate(sample_data):\n",
    "    print(f\" Sample{i+1}:\")\n",
    "    # Fix: Use example['text'] instead of sample['text']\n",
    "    print(f\" Text: {example['text'][:100]}...\")\n",
    "    print(f\" label: {example['label']} ({'Positive' if example['label'] == 1 else 'Negative'})\")\n",
    "    print()\n",
    "\n",
    "  # LABEL DISTRIBUTUION - Moved inside the function\n",
    "  train_labels = dataset['train']['label']\n",
    "  positive_count = sum(train_labels)\n",
    "  # Fix: calculate negative_count correctly\n",
    "  negative_count = len(train_labels) - positive_count\n",
    "  print(\"4. Label distribution:\")\n",
    "  print(f\"   - Positive reviews: {positive_count:,} ({positive_count/len(train_labels)*100:.1f}%)\")\n",
    "  print(f\"   - Negative reviews: {negative_count:,} ({negative_count/len(train_labels)*100:.1f}%)\")\n",
    "\n",
    "  return dataset\n",
    "\n",
    "dataset = load_and_explore_data()\n",
    "print(\"Data set loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c852aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model intialization\n",
    "print(\"Intialize BERT model\")\n",
    "print(\"=\" *35)\n",
    "\n",
    "def initialize_bert_model():\n",
    "  \"\"\"Intializing DistilBERT model and tokenizer with professional setup\"\"\"\n",
    "  model_name=\"distilbert-base-uncase\"\n",
    "\n",
    "  print(f\"1. Loading tokenizer: {model_name}\")\n",
    "  tokenizer = AutoTokenizer.form_pretrained(model_name)\n",
    "\n",
    "  print(f\"2. Loading pre-trained model...\")\n",
    "  model = AutoModelForSequenceClassification.form_pretrained(model_name,\n",
    "                                                             num_labels=2,\n",
    "                                                             id2label={0: \"Negative\", 1: \"Positive\"},\n",
    "                                                             label2id={\"Negative\": 0, \"Positive\": 1}\n",
    "                                                             )\n",
    "  print(\"3. Model architecture overview:\")\n",
    "  print(\"f Model type: {model.__class__.__name__}\")\n",
    "  print(f\" Number of parameters: {model.num_parameters():,}\")\n",
    "  print(f\" Number of labels: {model.config.num_labels}\")\n",
    "  \n",
    "  print(\"4. Moving model to GPU...\")\n",
    "  if torch.cuda.is_available:\n",
    "    model=model.to('cuda')\n",
    "    print(\" Model succesfully moved to GPU\")\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "  # intialize model\n",
    "  tokenizer, model = initialize_bert_model()\n",
    "  print(\"BERT model intialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133960f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Data Preprocessing & Tokenization\n",
    "print(\"DATA PREPROCESSING & TOKENIZATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def preprocess_data(tokenizer, dataset):\n",
    "    \"\"\"Tokenize and prepare dataset for training\"\"\"\n",
    "    print(\"1. Defining tokenization function...\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Return as PyTorch tensors\n",
    "        return tokenizer(\n",
    "            examples[\"text\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    print(\"2. Tokenizing training dataset...\")\n",
    "    tokenized_train = dataset[\"train\"].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000\n",
    "    )\n",
    "\n",
    "    print(\"3. Tokenizing test dataset...\")\n",
    "    tokenized_test = dataset[\"test\"].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        batch_size=1000\n",
    "    )\n",
    "\n",
    "    print(\"4. Dataset overview after tokenization:\")\n",
    "    print(f\"   - Training features: {list(tokenized_train.features.keys())}\")\n",
    "\n",
    "    # --- Diagnosis: Inspect the output of tokenization for a single sample ---\n",
    "    print(\"\\n--- Diagnosis of tokenized output ---\")\n",
    "    first_example_input_ids = tokenized_train[0]['input_ids']\n",
    "    print(f\"Type of tokenized_train[0]['input_ids']: {type(first_example_input_ids)}\")\n",
    "    if isinstance(first_example_input_ids, list):\n",
    "        print(f\"Length of the list: {len(first_example_input_ids)}\")\n",
    "        if len(first_example_input_ids) > 0:\n",
    "            print(f\"Type of the first element in the list: {type(first_example_input_ids[0])}\")\n",
    "            # If the first element is a tensor, get its shape\n",
    "            if isinstance(first_example_input_ids[0], torch.Tensor):\n",
    "                 print(f\"Shape of the first element (tensor): {first_example_input_ids[0].shape}\")\n",
    "            else:\n",
    "                # If the first element is not a tensor, try to convert and print shape\n",
    "                try:\n",
    "                    temp_tensor = torch.tensor(first_example_input_ids[0])\n",
    "                    print(f\"Shape after converting first element to tensor: {temp_tensor.shape}\")\n",
    "                except:\n",
    "                    print(\"Could not convert the first element to a tensor.\")\n",
    "    else:\n",
    "        # If it's not a list, assume it's a tensor and print its shape\n",
    "        print(f\"Input shape: {first_example_input_ids.shape}\")\n",
    "    print(\"-------------------------------------\")\n",
    "    # --- End of Diagnosis ---\n",
    "\n",
    "    # Original line causing error - will be skipped in diagnosis phase\n",
    "    # print(f\"   - Input shape: {tokenized_train[0]['input_ids'].shape}\")\n",
    "\n",
    "\n",
    "    return tokenized_train, tokenized_test\n",
    "\n",
    "# Preprocess data\n",
    "tokenized_train, tokenized_test = preprocess_data(tokenizer, dataset)\n",
    "print(\"DATA PREPROCESSING COMPLETED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Configuration\n",
    "print(\" TRAINING CONFIGURATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "def setup_training():\n",
    "    \"\"\"Configure professional training parameters\"\"\"\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        # Output settings\n",
    "        output_dir=\"./results\",\n",
    "        overwrite_output_dir=True,\n",
    "\n",
    "        # Training parameters\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "\n",
    "        # Evaluation settings\n",
    "        # Changed evaluation_strategy to eval_strategy\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "\n",
    "        # Logging\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=500,\n",
    "        report_to=\"none\",\n",
    "\n",
    "        # Optimization\n",
    "        warmup_steps=500,\n",
    "        fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    )\n",
    "\n",
    "    print(\" TRAINING CONFIGURATION:\")\n",
    "    print(f\"   - Epochs: {training_args.num_train_epochs}\")\n",
    "    print(f\"   - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "    print(f\"   - Learning rate: {training_args.learning_rate}\")\n",
    "    print(f\"   - Evaluation: {training_args.eval_strategy}\") # Changed to eval_strategy\n",
    "    print(f\"   - FP16 (speedup): {training_args.fp16}\")\n",
    "\n",
    "    return training_args\n",
    "\n",
    "training_args = setup_training()\n",
    "print(\" TRAINING CONFIGURATION COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd23aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation Metrics & Trainer\n",
    "print(\"EVALUATION METRICS SETUP\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute comprehensive evaluation metrics\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1_score\": f1,\n",
    "        \"error_rate\": 1 - accuracy\n",
    "    }\n",
    "\n",
    "print(\"1. Setting up Trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\"2. Trainer configuration:\")\n",
    "print(f\"   - Training samples: {len(tokenized_train):,}\")\n",
    "print(f\"   - Evaluation samples: {len(tokenized_test):,}\")\n",
    "print(f\"   - Total steps: {training_args.num_train_epochs * len(tokenized_train) // training_args.per_device_train_batch_size:,}\")\n",
    "\n",
    "print(\"TRAINER SETUP COMPLETE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c663c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: MODEL TRAINING EXECUTION\n",
    "print(\"ðŸ‹ï¸ STARTING MODEL TRAINING\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"1.  TRAINING OVERVIEW:\")\n",
    "print(f\"   - Model: DistilBERT (BERT's faster cousin)\")\n",
    "print(f\"   - Task: Learn if movie reviews are Positive or Negative\")\n",
    "print(f\"   - Data: 25,000 movie reviews\")\n",
    "print(f\"   - Time: 20-30 minutes with GPU\")\n",
    "\n",
    "print(\"\\n2. TRAINING STARTED...\")\n",
    "print(\"   This is where your AI learns from the data!\")\n",
    "print(\"   The coach (Trainer) is now teaching your model...\")\n",
    "print(\"   Grab a coffee!\")\n",
    "\n",
    "# THIS IS WHERE MAGIC HAPPENS!\n",
    "training_results = trainer.train()\n",
    "\n",
    "print(\"\\n3. TRAINING COMPLETED!\")\n",
    "print(\"=\" * 30)\n",
    "print(\"   Your AI has finished learning!\")\n",
    "print(\"   Now let's see how well it learned...\")\n",
    "\n",
    "# Show training results\n",
    "print(f\"   - Total time: {training_results.metrics['train_runtime']:.0f} seconds\")\n",
    "print(f\"   - Final loss: {training_results.metrics['train_loss']:.4f}\")\n",
    "print(f\"   - Speed: {training_results.metrics['train_samples_per_second']:.1f} samples/second\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ PHASE 1 COMPLETE: MODEL IS TRAINED!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba7114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "print(\"TESTING AI KNOWLEGDE\")\n",
    "print(\"=\" *35)\n",
    "\n",
    "print(\"1.  FINAL EXAM TIME!\")\n",
    "print(\"   Now testing your model on 25,000 NEW reviews\")\n",
    "print(\"   It has never seen these before!\")\n",
    "\n",
    "# test the model\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n2.  EXAM RESULTS:\")\n",
    "print(\"   \" + \"=\"*20)\n",
    "accuracy = eval_results['eval_accuracy']\n",
    "f1 = eval_results['eval_f1_score']\n",
    "\n",
    "print(f\"   Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"    F1-Score: {f1:.4f}\")\n",
    "print(f\"    Error Rate: {1-accuracy:.4f} ({(1-accuracy)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n3.  GRADE INTERPRETATION:\")\n",
    "if accuracy > 0.92:\n",
    "    print(\"    EXCELLENT! Your AI is smarter than 92% of humans!\")\n",
    "elif accuracy > 0.88:\n",
    "    print(\"    VERY GOOD! Professional-level performance!\")\n",
    "elif accuracy > 0.85:\n",
    "    print(\"    GOOD! Solid understanding of sentiment!\")\n",
    "else:\n",
    "    print(\"    NEEDS PRACTICE! But still better than guessing!\")\n",
    "\n",
    "print(f\"\\n4. RANDOM GUESS COMPARISON:\")\n",
    "print(f\"   - Random guessing: 50.00%\")\n",
    "print(f\"   - Your AI: {accuracy*100:.2f}%\")\n",
    "print(f\"   - Improvement: +{(accuracy-0.5)*100:.2f}%\")\n",
    "\n",
    "print(\"\\n PHASE 2 COMPLETE: MODEL EVALUATED!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
